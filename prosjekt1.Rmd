---
subtitle: "TMA4268 Statistical Learning V2018"
title: "Compulsory exercise 1: Group 31"
author: "Øyvind Auestad and Erik Dengerud"
date: "16.02.2017"
output: #3rd letter intentation hierarchy
  # prettydoc::html_pretty:
  #   theme: tactile
  #   highlight: github
  pdf_document:
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(GGally)
library(class)
library(MASS)
library(pROC)
```

\section*{Problem 1 - Core concepts in statistical learning}

\subsection{a) Training and test MSE}

* We observe from the figure that small values of $K$ gives a model whose mean over 1000 repetitions is close to the true function in shape. For larger values of $K$, we see that the model fails at the ends of the interval. We also see that the variance is high for small values of $K$, and smaller for larger ones. This is due to the bias-variance tradeoff, so the models are under- and over-fitted. 
A part of the interval close to the endpoints is observed to be constant for large values of $K$. This is because the k-nearest-neighbours algorithm is used to fit the model and the neighbours are the same for the points in this part of the interval for larger values. This contributes to the high bias in these models. This problem does not arise for smaller values since the number of neighbours used is less. 
  
* Small values of $K$ gives the most flexible fit. A small value of $K$ has a high variance, lower bias, and a risk of overfitting. 

* From the figure we observe that MSE on the training set is increasing for all values of $K$. This is due to over-fitting on the low values of $K$. On the test set, we observe that the curve has a dip after the first few values, and then increases for every $K$. The trend from the single traing set fits well to the rest of the training sets. This can also be said about the test sets, where they all point to a good choise of $k$ being around $4$ for this data.

* When evaluating the model, we fit our model to the training data and then evaluate it on the test set to find the a fitting flexibility. From the 1000 repetitions it seems that a $K$ around 4 results in the minimal MSE and this would thus be our choise of $k$. Since the 1000 repetitions somewhat concur on this value, this appears to be a good choise. 

\subsection{b) Bias-variance trade-off}

* The squared bias and variance is calculated from the predicted values and the true values at each $x$ in the following way. For each $m \in M$, a model is fitted. this model is then used to predict a value at each $x$. We now have 1000 predicted values at each $x$ and it is with these values we calculate the squared bias as we do with MSE and the variance at each $x$.

* We look at the figure and observes what happens when the flexibility increases (K decreases). The squared bias decreases strictly to zero as $K$ approaches zero. This is because the model is overfitted for small $K$ as stated before. The variance increases as the flexibility increases. This is a consequence of the bias-variance trade off. The irreducible error is constant as this comes from the error term and can not be rerduced by the choice of the model. 

* A good model has as little bias and variance as possible. From the figure, this is obtained at a value around three to five where three seems to be the lowest. This corresponds well with what was found earlier.

* It seems from the figure that the curves has a minimum at around $K=10$. This is higher than what has been found earlier. If we return to figure 2, we see that this model is a good fit with low variance in the middle part of the interval. The difference between the model and the true function is higher near the endpoints, and this is what gives this model a higher MSE. The plots in figure 4 have values in the interval $[-2,2.5]$. This leads to a total at these values that is less than what we found earlier. The trend discussed is seen as the value $x_0=2.5$ makes the total skyrocket for higher values of $K$. It is clear that the optimal value of $K$ is dependent on the domain we think is the most relevant. This in turn depends on what the model should be used for. Thus $K=3$ seems to be a good choice if one needs a good fit also at the endpoints, but a choice og $K=10$ is better if one only need a good fit on the middle part of the interval.


\section*{Problem 2 - Linear regression} 
```{r, echo = FALSE}
data = read.table("https://www.math.ntnu.no/emner/TMA4268/2018v/data/SYSBPreg3uid.txt")
modelA = lm(-1/sqrt(SYSBP) ~ ., data = data)
summary(modelA)
```

\section*{a)}

\begin{itemize}

\item The fittet model has the equation $\hat{Y} = X\hat{\beta}$, where $\hat{\beta} = (X^TX)^{-1}X^TY$, X is the $n \times (p + 1)$ design matrix, and Y is the corresponding response values.

\item "Estimate" is the estimated coeffictients obtaining the minimum residual square error with the data set. The "intercept" is the constant term in the regression model.

\item The "standard error" is the estimated standard deviation in the estimated coefficients. It is given as the square root of $\hat{Var}(\hat{{\beta}_{j}}) = c_{jj}{\hat{\sigma}}^2$, where $c_{ij} = ((X^TX)^{-1})_{ij}$ and ${\hat{\sigma}}^2 = (Y - \hat{Y})^T(Y - \hat{Y}) / (n - p - 1)$.

\item The "t value" is for every coefficent $j$, $\frac{ \hat{{\beta}_j} }{ \sqrt{c_{jj}} \hat{ \sigma } }$ which is t distributed with $n - p - 1$ degrees of freedom under the assumtion that $H_0$ is true, that is, ${\beta}_j$ truly is $0$. "Pr(t > |t|)" is then the probability of obeserving such an extreme t value given that $H_0$ is true. Hence Pr(>|t|) $:= P(|T_{n - p - 1}| \geq |\frac{ \hat{{\beta}_j} }{ \sqrt{c_{jj}} \hat{ \sigma } }|) = 2P(T_{n - p - 1} \geq |\frac{ \hat{{\beta}_j} }{ \sqrt{c_{jj}} \hat{ \sigma } }|)$.

\item The "Residual standard error" is our estimate for the standard deviation of Y. The standard error squared is given as ${ \hat{\sigma} }^2 = (Y - \hat{Y})^T(Y - \hat{Y}) / (n - p - 1)$.

\item The "F - statistic" is used to check the hypothesis of all betas being 0. In the table it is given as $\frac{(TSS - RSS) / p} {RSS / (n - p - 1)}$, which is Fisher distributed with degrees of freedom $p$ and $n - p - 1$, where $TSS := \sum_{i = 1}^{n}(y_i - \bar{y})^2$, and $RSS := \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2$.

\end{itemize}

\subsection{b)}

\begin{itemize}

\item The proportion of variability explained by the model is given by the $R^2 -$ statistic $:= (TSS - RSS) / TSS$, here being equal to 0.2494. Hence our model explains approximately $25\%$ of the variance in the response value.

\end{itemize}
```{r, echo = FALSE}
# residuls vs fitted
p1 <- ggplot(modelA, aes(.fitted, .resid)) + geom_point(pch = 21) + 
  geom_hline(yintercept = 0, linetype = "dashed") + 
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") + 
  labs(x = "Fitted values", y = "Residuals", title = "Fitted values vs. residuals",
       subtitle = deparse(modelA$call))
# qq-plot of residuals
p2 <- ggplot(modelA, aes(sample = .stdresid)) +
  stat_qq(pch = 19) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals", title = "Normal Q-Q", subtitle = deparse(modelA$call))
p1
p2
```
\begin{itemize}

\item Looking at the plot of residuals vs. fitted values we note that it does not appear to be a correlation between the value of the response and the variance of the response, and the mean appears to be 0. This fits well with the assumtion of the noise being normally distributed with mean 0 and constant variance.

The QQ-plots strengthens our belief in this assumtion, as the points form a linear line.

\end{itemize}

```{r, echo = FALSE}
modelB <- lm(SYSBP ~ ., data = data)
summary(modelB)
# residuls vs fitted
p1 <- ggplot(modelB, aes(.fitted, .resid)) + geom_point(pch = 21) + 
  geom_hline(yintercept = 0, linetype = "dashed") + 
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") + 
  labs(x = "Fitted values", y = "Residuals", title = "Fitted values vs. residuals",
       subtitle = deparse(modelB$call))
# qq-plot of residuals
p2 <- ggplot(modelB, aes(sample = .stdresid)) +
  stat_qq(pch = 19) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals", title = "Normal Q-Q", subtitle = deparse(modelB$call))
p1
p2
```

\begin{itemize}

\item Looking at the diagnostic plots of model B we note that the values in the residuals vs. fitted values plot does not appear to be scattered evenly around the $x$-axis, where the greatest deviances appear to be in the postitive half plane. The QQ-plot also suggests that these residuals are not normally distributed, and thus the regression model fails, and the inference in the summary is not valid. We note that the RSE is considerably larger as well, but since the response is different, it is hard to directly compare. Clearly we prefer model A to make inference about systolic blood pressure, for this model follows the regression error assumptions, where model B fails.

\end{itemize}

\subsection*{c)}

\begin{itemize}

\item The estimate for ${\hat{\beta}}_{BMI}$ is $3.087\cdot 10^{-4}$. 

\item We interpret the estimated coefficient ${\hat{\beta}}_{BMI}$ as the coefficient of the variable containing the value of BMI in the linear expression for $-1 / \sqrt{SYSBP}$, that is, the impact of change in BMI on the response

\[
{\hat{\beta}}_{BMI} = \frac{\partial (-1/\sqrt{SYSBP)}}{\partial BMI}
\]

\item Since ${\hat{\beta}}_{BMI} \sim N({\beta}_{BMI}, {\sigma}^2c_{BMI})$, where $c_{BMI} := \textrm{diagonal entry corresponding to BMI of }(X^TX)^{-1}$ we have

\[
\frac{ {(\hat{\beta}}_{BMI} - {\beta}_{BMI}) / (\sigma \sqrt{c_{BMI}}) }{ \sqrt{ \frac{1}{{\sigma}^2} RSS / (n - p - 1) } } = \frac{ {\hat{\beta}}_{BMI} - {\beta}_{BMI} }{\sqrt{\frac{RSS}{n - p - 1}c_{BMI}}} \sim T_{n - p - 1}
\]

It follows that

\[
Pr({\beta}_{BMI} \in ({\hat{\beta}}_{BMI} - {\hat{\sigma}} \sqrt{c_{BMI}} t_{0.995, 2593}, {\hat{\beta}}_{BMI} - {\hat{\sigma}} \sqrt{c_{BMI}} t_{0.005, 2593} )) = 0.99
\]

Setting $t_{0.005, 2593} = -2.577727$ and $t_{0.995, 2593} = 2.577727$, we compute the interval to be $(2.325282\cdot 10^{-4}, 3.848718\cdot 10^{-4})$. This interval tells us that with probability $0.99$, the true value of the coefficient is contained in this interval.

\item We note that if $H_0$ is true, the center of the t distribution for prediction of $\hat{\beta}_{BMI}$ would be $0$, but the degrees of freedom the same as for this prediction. Hence, a $99\%$ prediction interval for the estimated coefficient would in this case be $(-|2.325282\cdot 10^{-4} -  3.087\cdot 10^{-4}|, |3.848718\cdot 10^{-4} - 3.087\cdot 10^{-4}|) = (-7.61718\cdot 10^{-5}, 7.61718\cdot 10^{-5})$. Clearly our observed value is outside the interval, meaning that the p value must be less than or equal to $0.01$.

\end{itemize}

\section*{d)}

\begin{itemize}

\item Model A predicts the response of these values to be $-0.08667246$, which corresponds to a SYSBP of $133.1183$. 

\item Let $\tilde{Y_0}$ be a new observation of $-1 / \sqrt{SYSBP}$ corresponding to the point ${x_0}$. Since we have $\tilde{Y_0} - {x_0}^T\beta \sim N(0, {\sigma}^2(1 + {x_0}^T(X^TX)^{-1}{x_0}))$ we get

\[
\frac{(\tilde{Y_0} - {x_0}^T\hat{\beta}) / (\sigma \sqrt{1 + {x_0}^T(X^TX)^{-1}x_0})}{\sqrt{\frac{1}{{\sigma}^2} RSS / (n - p - 1)}} = \frac{ \tilde{Y_0} - {x_0}^T\hat{\beta} }{ \hat{\sigma}\sqrt{1 + {x_0}^T(X^TX)^{-1}x_0} } \sim T_{n - p - 1}
\]

letting $\tilde{Y_0} = -\frac{1}{\sqrt{Y_0}}$ we obtain the following prediction interval for SYSBP at $x_0$

\[
Pr(Y_0 \in \bigg( \frac{1} {({x_0}^T\hat{\beta} + \hat{\sigma}kt_{0.05, 2593})^2 }, \frac{1} {({x_0}^T\hat{\beta} + \hat{\sigma} kt_{0.95, 2593})^2}\bigg) ) = 0.90, \\ k = \sqrt{1 + {x_0}^T(X^TX)^{-1}x_0}
\]

Setting $t_{0.05, 2593} = -1.645441$ and $t_{0.95, 2593} = 1.645441$ we compute the following prediction interval $(107.9250, 168.2845)$. 

\item This interval is very large numericaly but also in the state of the person having this blood pressure. It ranges from healty to close to lethal, and it is just a $90\%$ prediction interval. In other words it is not particularly useful. 

\end{itemize}

\section*{Problem 3 - Classification}

\section*{a)}

\begin{itemize}

\item We want to show that $\textrm{logit}(p_i)=\log(\frac{p_i}{1-p_i})$ is a linear function, where $p_i=\frac{e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}{1+e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}$. We see that 

\[
1-p_i=1-\frac{e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}{1+e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}=\frac{1}{1+e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}.
\]

and thus

\[
\textrm{logit}(p_i)=\log\bigg(\frac{p_i}{1-p_i}\bigg)=\log\Bigg(\frac{\frac{e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}{1+e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}}{\frac{1}{1+e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}}\Bigg)= \log(e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}})=\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}.
\]

So $\textrm{logit}(p_i)$ is linear.

\end{itemize}

```{r, echo = FALSE, warning=FALSE, message=FALSE}
wine = read.csv("https://www.math.ntnu.no/emner/TMA4268/2018v/data/Comp1Wine.csv",sep=" ")
wine$class = as.factor(wine$class-1)
colnames(wine) = c("y","x1","x2")

n = dim(wine)[1]
set.seed(1729) 
ord = sample(1:n) #shuffle
test = wine[ord[1:(n/2)],]

train = wine[ord[((n/2)+1):n],]

fit <- glm(y ~., family="binomial",data=train)
summary(fit)
```

\begin{itemize}

\item $\hat{\beta_1}$ and $\hat{\beta_2}$ can be interpreted as how the odds vary with $x_{i1}$, $x_{i2}$ respectively. The odds is given as $\frac{p_i}{1-p_i}$. If the covariate $x_{i1}$ is increased by one unit, the odds is multiplied by $\exp(\beta_1)$. The same is true for $x_{i2}$ and $\exp(\beta_2)$. $hat{\beta}_i, \ i = 0, 1, 2$ and are estimates for the parameters $\beta$ in the model, and are estimated by maximum likelihood on the training data.

\item We find the formula for the class boundary by solving $\hat{Pr}(Y = 1| X) = 0.5$. This gives 

\[
  \frac{e^{\hat{\beta_0}+\hat{\beta_1}x_{i1}+\hat{\beta_2} x_{i2}}}{1+e^{\hat{\beta_0}+\hat{\beta_1}x_{i1}+\hat{\beta_2} x_{i2}}} = 0.5,
\]

so

\[
  0.5e^{\hat{\beta_0}+\hat{\beta_1}x_{i1}+\hat{\beta_2} x_{i2}}=0.5.
\]

This means that we need $\hat{\beta_0}+\hat{\beta_1}x_{i1}+\hat{\beta_2}x_{i2} = 0$. Thus

\[
x_2 = -\frac{\hat{\beta_0}}{\hat{\beta_2}} - \frac{\hat{\beta_1}}{\hat{\beta_2}}x_1,
\]

and we see that the boundary is linear.

\item The training data is plotted with the class boundary.

\end{itemize}

```{r, echo = FALSE}
slope = -fit$coefficients[2] / fit$coefficients[3]
intercept = -fit$coefficients[1] / fit$coefficients[3]
g1 = ggplot(data = train, aes(x = x1, y = x2, color = y)) + geom_point(data = train, pch = 3) +
  geom_abline(slope = slope, intercept = intercept) + ggtitle("Training data and logistic boundary")

g1
```

\begin{itemize}

\item From the summary we find that $\hat{\beta_0} = 2.1431$, $\hat{\beta_1} = 0.3245$, $\hat{\beta_2} = -1.9216$. The probability of class 1 given $x_1 = 17$ and $x_2 = 3$ is then

\[
p = \frac{e^{\hat{\beta_0}+\hat{\beta_1}x_{1}+\hat{\beta_2} x_{2}}}{1+e^{\hat{\beta_0}+\hat{\beta_1}x_{1}+\hat{\beta_2} x_{2}}} = 0.8693.
\]

The interpretation of this is that based on the model, the probability of this point belonging to class one is $86.9\%$.

\item The predicted probabilities for the test set is visualized in a confusion matrix with a cut-off of $0.5$. The sensitivity is then $22 / 27 = 0.8148$, and specificity is $33 / 38 = 0.8684$. Since the sensitivity and specificity is high, the classification model appears to fit the dataset well. Perhaps a linear boundary is fitting.

\end{itemize}

```{r, echo = FALSE}
X = data.matrix(test)
Y <- X[,1]
X[,1] = 1
betahat = as.matrix(fit$coefficients)
dot = X %*% betahat
predglm = exp(dot) / (1 + exp(dot))
testclass = ifelse(predglm > 0.5, 1, 0)
t = table(test$y, testclass)
t
```

\section*{b)}

\begin{itemize}

\item The expression

\[
P(Y = j | X = x_0) = \frac{1}{K} \sum_{i \in N_0} I(y_i = j)
\]

returns the proporiton of the $K$ nearest neighbours of $x_0$ belonging to class $j$. $K$ is the number of neighbours we are considering, and the set $N_0$ contains all these neigbours. $I(y_i = j)$ is an indictor function taking the value $1$ when the argument is true, and $0$ otherwise.

\item We have fitted the model below.

\item For $K = 3$, the confusion table is shown below. The sensitivity and specificty is respectively $23 / 27 = 0.8519$ and $29 / 38 = 0.7632$. It scores a bit lower than classification by logistic regression, but it appears to perform ok.

\end{itemize}

```{r}
knn3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = FALSE)
t <- table(test$y, knn3)
t
```

\begin{itemize}

\item We repeat with $K = 9$. The confusion table is shown below. The sensitivity and specificity is now respectively $23 / 27 = 0.8519$ and $31 / 38 = 0.8158$. This model has higher sensitivity and specificity than the $3$-nearest neighbour model, and we therefore prefer this one. If we choose $K$ very small, we risk overfitting the data, and with very large $K$ the model is possibly not flexible enough. We therefore search for the optimal $K$.

\end{itemize}

```{r}
knn9 = knn(train = train[,-1], test = test[,-1], k = 9, cl = train$y, prob = FALSE)
t <- table(test$y, knn9)
t
```

\section*{c)}

\begin{itemize}

\item ${\pi}_k$ is the probability of an observation being from class $k$, that is, the probability of getting a sample from a certain wine, wine $1$ or $2$ in this case. ${\mu}_k$ is the expected value of a point from class $k$, in our case the expected values of $(x1, x2)$ corresponding to wine $1$ and wine $2$. $\Sigma$ is the variance matrix of the distribution of a class, here assumed to be equal for every class. Hence we assume that the variance in observations of $(x1, x2)$ are the same for both wine $1$ and $2$. $f_k(x)$ is the distribution of points $(x1, x2)$, coming from class $k$, i. e. wine $1$ and $2$, which we assume takes the form of the normal distribution with mean ${\mu}_k$ and variance $\Sigma$.

\end{itemize}

```{r, include = FALSE}
X <- data.matrix(train)
n <- length(X[,1])
X1 = c() 
X2 = c()
for (i in 1:n) {
  if (X[i,1] == 1) {
    X1 = c(X1, X[i,2:3])
  } else {
    X2 = c(X2, X[i,2:3])
  }
}
X1 = matrix(X1, byrow = TRUE, ncol = 2)
X2 = matrix(X2, byrow = TRUE, ncol = 2)
n1 <- length(X1[,1]); n2 <- length(X2[,1])

mu1hat <- t(X1) %*% rep(1, n1) / n1
mu2hat <- t(X2) %*% rep(1, n2) / n2

X1 = X1 - matrix(rep(mu1hat, n1), byrow = TRUE, ncol = 2)
X2 = X2 - matrix(rep(mu2hat, n2), byrow = TRUE, ncol = 2)
S1 = t(X1) %*% X1
S2 = t(X2) %*% X2

S <- (S1 + S2) / (n - 2)
```

\begin{itemize}

\item To estimate ${\pi}_k$ we consider the proportion of observations coming from class $k$, that is, $\hat{\pi}_k = \frac{n_k}{n}$. We compute $\hat{\pi}_1 = 32 / 65 = 0.4923$ and $\hat{pi}_2 = 33 / 65 = 0.5077$. To estimate ${\mu_k}$ we consider the estimated mean of points coming from class $k$, that is $\hat{\mu}_k = \frac{1}{n_k} \sum_{i, y_i = k} x_i$, which we compute to be: $\hat{\mu}_1 = (16.7781, 5.4575)^T$, $\hat{\mu}_2 = (19.6879, 3.0536)^T$. To estimate $\Sigma$ we consider the estimated variance for each class, $\hat{\Sigma}_k := \frac{1}{n_k - 1}\sum_{i, y_i = k} (X_i - \hat{\mu}_k)(X_i - \hat{\mu}_k)^T$, and compute: 
\[
\hat{\Sigma} = \sum_{k = 1}^2 \frac{n_k - 1}{n - 2} \hat{\Sigma}_k = 
\begin{bmatrix}
  6.2014 & -0.4447 \\
  -0.4447 & 1.1678
\end{bmatrix}
\]

\item The desicion boundary is given by the equality $P(Y = 0 | X) = P(Y = 1 | X)$, that is

\[
\frac{\pi_0 f_0(x)}{\sum_{i = 0}^{1}\pi_kf_i(x)} = \frac{\pi_1 f_1(x)}{\sum_{i = 0}^{1}\pi_kf_i(x)}
\]

which simplifies to 

\[
\pi_0 \exp(-\frac{1}{2}(x - \mu_0)^T{\Sigma}^{-1}(x - \mu_0)) = \pi_1 \exp(-\frac{1}{2}(x - \mu_1)^T{\Sigma}^{-1}(x - \mu_1))
\]

taking the logarithm on both sides yields

\[
\log(\pi_0) - \frac{1}{2}({\mu_0}^T{\Sigma}^{-1}\mu_0 - 2{\mu_0}^T{\Sigma}^{-1}x + x^T{\Sigma}^{-1}x) = \log(\pi_1) - \frac{1}{2}({\mu_1}^T{\Sigma}^{-1}\mu_1 - 2{\mu_1}^T{\Sigma}^{-1}x + x^T{\Sigma}^{-1}x) 
\]

and finally

\[
\log(\pi_0) - \frac{1}{2}{\mu_0}^T{\Sigma}^{-1}\mu_0 + {\mu_0}^T{\Sigma}^{-1}x = {\delta}_0(x) = \log(\pi_1) - \frac{1}{2}{\mu_1}^T{\Sigma}^{-1}\mu_1 + {\mu_1}^T{\Sigma}^{-1}x = {\delta}_1(x)
\]

\item We note that this descision rule is the same as classifying to the class with highest probability. Let $\hat{\delta}_k(x) := \log(\hat{\pi_k}) - \frac{1}{2}{\hat{\mu_k}}^T{\Sigma}^{-1}\hat{\mu_k} + {\hat{\mu_k}}^T{\hat{\Sigma}}^{-1}x$. And so, by the previous task, we have

\[
\hat{\delta}_0(x) = \hat{\delta}_1(x)
\]

and the boundary becomes

\[
\log(\hat{\pi_0}) + \frac{1}{2}{\hat{\mu_0}}^T{\Sigma}^{-1}\hat{\mu_0} - {\hat{\mu_0}}^T{\hat{\Sigma}}^{-1}x = \log(\hat{\pi_1}) + \frac{1}{2}{\hat{\mu_1}}^T{\Sigma}^{-1}\hat{\mu_1} - {\hat{\mu_1}}^T{\hat{\Sigma}}^{-1}x
\]

that is

\[
\frac{1}{2}({\hat{\mu}_0}^T {\hat{\Sigma}}^{-1}\hat{\mu_0} - {\hat{\mu}_1}^T {\hat{\Sigma}}^{-1}\hat{\mu_1}) + \log\bigg(\frac{\hat{\pi}_0}{\hat{\pi}_1}\bigg) + ({\hat{\mu}_1}^T{\hat{\Sigma}}^{-1} - {\hat{\mu}_0}^T{\hat{\Sigma}}^{-1})x = 0
\]

Inserting our estimates from the training set we get the boudary

\[
x_2 = 0.1711x_1 + 1.1205
\]

\item The descision boundary with both the training and test observations is shown below (circles are from the training set)

\end{itemize}

```{r, echo = FALSE}
Sinv <- solve(S)
pi1hat <- n1 / n; pi2hat <- n2 / n
b <- 0.5 * (t(mu1hat) %*% Sinv %*% mu1hat - t(mu2hat) %*% Sinv %*% mu2hat) + log(pi1hat / pi2hat) 
a <- t(mu2hat) %*% Sinv - t(mu1hat) %*% Sinv
slope <- -a[1] / a[2]; intercept <- -b / a[2]
p <- ggplot() + geom_point(data = train, aes(x = x1, y = x2, color = y), pch = 1) +
  geom_point(data = test, aes(x = x1, y = x2, color = y), pch = 3) + geom_abline(slope = slope, intercept = intercept)

p
```

\begin{itemize}

\item Done below

\end{itemize}

```{r}
wine_lda <- lda(y ~ x1 + x2, data = train)
```

\begin{itemize}

\item The confusion table is show below. We get a sesitivity of $22 / 27 = 0.8148$, and a specificity of $34 / 38 = 0.8947$. The performance is good compared to the logistic regression and KNN. Perhaps the data fit a linear model, and the normal distribution assumtion is not so far off. 

\end{itemize}

```{r, echo = FALSE}
predicted = predict(wine_lda, newdata = test)$class
t = table(test$y, predicted)
t
```

\begin{itemize}

\item The most important diffence in regard to using LDA or QDA would be that with QDA we expect the variance of the classes to be different, and hence use different covariance matrices in their distributions. This allows for a more flexible fit to the data.

\end{itemize}

\section*{d)}

\begin{itemize}

\item To get in indication of the performance of the different classification methods, we list their sensitivity and specificity

\end{itemize}

```{r, echo = FALSE}
x <- matrix(c( 22/27, 33/38, 23/27, 31/38, 22/27, 34/38), byrow = TRUE, ncol = 2)
rownames(x) <- c("Logistic regression", "KNN (K = 9)", "LDA")
colnames(x) <- c("Sensitivity", "Specificity")
x
```

We note that over all, the methods based on a linear descision boundary scores the best, but the $9$-nearest neighbour is not far behind. The highest scoring is the LDA, and based on this table, this would be the preferred method. But since the race is so even it would be wrong to take any stong stance.

\begin{itemize}

\item A reciever operating characteristics curve is plot of sensitivity vs. specificity for a classifier, as we let the cut-off take on values in $[0, 1]$. An ROC curve is useful, since it indicates if there excist a particular cut-off giving both high sensitivity and specificity, that is, if the curve takes on values in the top left corner of the plot. Below are ROC plots for the three classifiers. We note that all three classifiers have points on their ROC curve close to the top left corner, but the two linear descision boundaries get closest. These methods also have the highest AUC value, that is, the area under the ROC curve, which suggests they are the over-all best classifiers for general cut-off. 

\end{itemize}

```{r, echo = FALSE, warning = FALSE}
glmroc=roc(response=test$y,predictor=predglm)
"Logistic regression"
plot(glmroc)
auc(glmroc)
KNN9 = knn(train = train[,-1], test = test[,-1], k = 9, cl = train$y, prob = F)
KNN9probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 9, cl = train$y, prob = TRUE))$prob
KNN9prob <- ifelse(KNN9 == "0", 1-KNN9probwinning, KNN9probwinning)
KNN9roc=roc(response=test$y,predictor=KNN9prob) 
"KNN (K = 9)"
plot(KNN9roc)
auc(KNN9roc)
ltrain=lda(y~x1+x2,data=train)
lpred=predict(object = ltrain, newdata = test)$posterior[,1]
lroc=roc(response=test$y,lpred)
"LDA"
plot(lroc)
auc(lroc)
```

