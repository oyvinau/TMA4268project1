---
subtitle: "TMA4268 Statistical Learning V2018"
title: "Compulsory exercise 1: Group X"
author: "NN1, NN and NN3"
date: "Date when you hand in"
output: #3rd letter intentation hierarchy
  # prettydoc::html_pretty:
  #   theme: tactile
  #   highlight: github
  pdf_document:
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("ggplot2")
```

\section*{Problem 2 - Linear regression} 
```{r, echo = FALSE}
data = read.table("https://www.math.ntnu.no/emner/TMA4268/2018v/data/SYSBPreg3uid.txt")
modelA = lm(-1/sqrt(SYSBP) ~ ., data = data)
summary(modelA)
```

\subsection*{a)}

\begin{itemize}

\item The fittet model has the equation $\hat{Y} = X\hat{\beta}$, where $\hat{\beta} = (X^TX)^{-1}X^TY$, X is the $n \times (p + 1)$ design matrix, and Y is the corresponding response values.

\item "Estimate" is the estimated coeffictients obtaining the minimum residual square error with the data set. The "intercept" is the constant term in the regression model.

\item The "standard error" is the estimated standard deviation in the estimated coefficients. It is given as the square root of $\hat{Var}(\hat{{\beta}_{j}}) = c_{jj}{\hat{\sigma}}^2$, where $c_{ij} = ((X^TX)^{-1})_{ij}$ and ${\hat{\sigma}}^2 = (Y - \hat{Y})^T(Y - \hat{Y}) / (n - p - 1)$.

\item The "t value" is for every coefficent $j$, $\frac{ \hat{{\beta}_j} }{ \sqrt{c_{jj}} \hat{ \sigma } }$ which is t distributed with $n - p - 1$ degrees of freedom under the assumtion that $H_0$ is true, that is, ${\beta}_j$ truly is $0$. "Pr(t > |t|)" is then the probability of obeserving such an extreme t value given that $H_0$ is true. Hence Pr(>|t|) $:= P(|T_{n - p - 1}| \geq |\frac{ \hat{{\beta}_j} }{ \sqrt{c_{jj}} \hat{ \sigma } }|) = 2P(T_{n - p - 1} \geq |\frac{ \hat{{\beta}_j} }{ \sqrt{c_{jj}} \hat{ \sigma } }|)$.

\item The "Residual standard error" is our estimate for the variance of Y. The standard error squared is given as ${ \hat{\sigma} }^2 = (Y - \hat{Y})^T(Y - \hat{Y}) / (n - p - 1)$.

\item The "F - statistic" is used to check the hypothesis of all betas being 0. In the table it is given as $\frac{(TSS - RSS) / p} {RSS / (n - p - 1)}$, which is Fisher distributed with degrees of freedom $p$ and $n - p - 1$, where $TSS := \sum_{i = 1}^{n}(y_i - \bar{y})^2$, and $RSS := \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2$.

\end{itemize}

\subsection{b)}

\begin{itemize}

\item The proportion of variability explained by the model is given by the $R^2 -$ statistic $:= (TSS - RSS) / TSS$, here being equal to 0.2494. Hence our model explains approximately $25\%$ of the variance in the response value.

\end{itemize}
```{r, echo = FALSE}
# residuls vs fitted
p1 <- ggplot(modelA, aes(.fitted, .resid)) + geom_point(pch = 21) + 
  geom_hline(yintercept = 0, linetype = "dashed") + 
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") + 
  labs(x = "Fitted values", y = "Residuals", title = "Fitted values vs. residuals",
       subtitle = deparse(modelA$call))
# qq-plot of residuals
p2 <- ggplot(modelA, aes(sample = .stdresid)) +
  stat_qq(pch = 19) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals", title = "Normal Q-Q", subtitle = deparse(modelA$call))
p1
p2
```
\begin{itemize}

\item Looking at the plot of residuals vs. fitted values we note that it does not appear to be a correlation between the value of the response and the variance of the response, and the mean appears to be 0. This fits well with the assumtion of the noise being normally distributed with mean 0 and constant variance.

The QQ-plots strengthens our belief in this assumtion, as the points form a linear line.

\end{itemize}

```{r, echo = FALSE}
modelB <- lm(SYSBP ~ ., data = data)
summary(modelB)
# residuls vs fitted
p1 <- ggplot(modelB, aes(.fitted, .resid)) + geom_point(pch = 21) + 
  geom_hline(yintercept = 0, linetype = "dashed") + 
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") + 
  labs(x = "Fitted values", y = "Residuals", title = "Fitted values vs. residuals",
       subtitle = deparse(modelB$call))
# qq-plot of residuals
p2 <- ggplot(modelB, aes(sample = .stdresid)) +
  stat_qq(pch = 19) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals", title = "Normal Q-Q", subtitle = deparse(modelB$call))
# p1
p2
```

\begin{itemize}

\item The RSE is considerably larger for model B, that is, the estimated variance in both the response and our estimated coefficients are larger in this model. Looking at the diagnostic plots of model B we also note that the QQ-plot suggests that these residuals are not normally distributed. Clearly we prefer model A to make inference about systolic blood pressure, for this model brings more likely coefficent estimates and follows the noise assumptions better.

\end{itemize}

\subsection*{c)}

\begin{itemize}

\item The estimate for ${\hat{\beta}}_{BMI}$ is $3.087\cdot 10^{-4}$. 

\item We interpret the estimated coefficient ${\hat{\beta}}_{BMI}$ as the coefficient of the variable containing the value of BMI in the linear expression for $-1 / \sqrt{SYSBP}$, that is, the impact of change in BMI on the response

\[
{\hat{\beta}}_{BMI} = \frac{\partial (-1/\sqrt{SYSBP)}}{\partial BMI}
\]

\item Since ${\hat{\beta}}_{BMI} \sim N({\beta}_{BMI}, {\sigma}^2c_{BMI})$, where $c_{BMI} := \textrm{diagonal entry corresponding to BMI of }(X^TX)^{-1}$ we have

\[
\frac{ {(\hat{\beta}}_{BMI} - {\beta}_{BMI}) / (\sigma \sqrt{c_{BMI}}) }{ \sqrt{ \frac{1}{{\sigma}^2} RSS / (n - p - 1) } } = \frac{ {\hat{\beta}}_{BMI} - {\beta}_{BMI} }{\sqrt{\frac{RSS}{n - p - 1}c_{BMI}}} \sim T_{n - p - 1}
\]

It follows that

\[
Pr({\beta}_{BMI} \in ({\hat{\beta}}_{BMI} - {\hat{\sigma}} \sqrt{c_{BMI}} t_{0.995, 2593}, {\hat{\beta}}_{BMI} - {\hat{\sigma}} \sqrt{c_{BMI}} t_{0.005, 2593} )) = 0.99
\]

Setting $t_{0.005, 2593} = -2.577727$ and $t_{0.995, 2593} = 2.577727$, we compute the interval to be $(2.325282\cdot 10^{-4}, 3.848718\cdot 10^{-4})$. This interval tells us that with probability $0.99$, the true value of the coefficient is contained in this interval.

\item We note that if $H_0$ is true, the center of the t distribution for prediction of $\hat{\beta}_{BMI}$ would be $0$, but the degrees of freedom the same as for this prediction. Hence, a $99\%$ prediction interval for the estimated coefficient would in this case be $(-|2.325282\cdot 10^{-4} -  3.087\cdot 10^{-4}|, |3.848718\cdot 10^{-4} - 3.087\cdot 10^{-4}|) = (-7.61718\cdot 10^{-5}, 7.61718\cdot 10^{-5})$. Clearly our observed value is outside the interval, meaning that the p value must be less than or equal to $0.01$.

\end{itemize}

\section*{d)}

\begin{itemize}

\item Model A predicts the response of these values to be $-0.08667246$, which corresponds to a SYSBP of $133.1183$. 

\item Let $\tilde{Y_0}$ be a new observation of $-1 / \sqrt{SYSBP}$ corresponding to the point ${x_0}$. Since we have $\tilde{Y_0} - {x_0}^T\beta \sim N(0, {\sigma}^2(1 + {x_0}^T(X^TX)^{-1}{x_0}))$ we get

\[
\frac{(\tilde{Y_0} - {x_0}^T\beta) / (\sigma \sqrt{1 + {x_0}^T(X^TX)^{-1}x_0})}{\sqrt{\frac{1}{{\sigma}^2} RSS / (n - p - 1)}} = \frac{ \tilde{Y_0} - {x_0}^T\beta }{ \hat{\sigma}\sqrt{1 + {x_0}^T(X^TX)^{-1}x_0} } \sim T_{n - p - 1}
\]

letting $\tilde{Y_0} = -\frac{1}{\sqrt{Y_0}}$ we obtain the following prediction interval for SYSBP at $x_0$

\[
Pr(Y_0 \in \bigg( \frac{1} {({x_0}^T\beta + \hat{\sigma}kt_{0.05, 2593})^2 }, \frac{1} {({x_0}^T + \hat{\sigma} kt_{0.95, 2593})^2}\bigg) ) = 0.90, \\ k = \sqrt{1 + {x_0}^T(X^TX)^{-1}x_0}
\]

Setting $t_{0.05, 2593} = -1.645441$ and $t_{0.95, 2593} = 1.645441$ we compute the following prediction interval $(107.9250, 168.2845)$. 

\item ??????????????????????????

\end{itemize}

\section{Problem 3 - Classification}

\subsection{a)}

\begin{itemize}

  \item We want to show that $$logit(p_i)=log(\frac{p_i}{1-p_i})$$ is a linear function, where $$p_i=\frac{e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}{1+e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}.$$ 
We see that $$1-p_i=1-\frac{e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}{1+e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}=\frac{1}{1+e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}.$$ 

Thus $$logit(p_i)=log\bigg(\frac{p_i}{1-p_i}\bigg)=log\Bigg(\frac{\frac{e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}{1+e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}}{\frac{1}{1+e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}}\Bigg)= log(e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}})=\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}.$$
So $logit(p_i)$ is linear.

\item 

\end{itemize}
```{r, echo = FALSE, warning=FALSE, message=FALSE}
library(GGally)
library(class)
library(MASS)
library(pROC)
wine=read.csv("https://www.math.ntnu.no/emner/TMA4268/2018v/data/Comp1Wine.csv",sep=" ")
wine$class = as.factor(wine$class-1)
colnames(wine) = c("y","x1","x2")
# p <- ggpairs(wine, ggplot2::aes(color=y))

n = dim(wine)[1]
set.seed(1729) #to get the same order if you rerun - but you change this to your favorite number
ord = sample(1:n) #shuffle
test = wine[ord[1:(n/2)],]

train = wine[ord[((n/2)+1):n],]

fit <- glm(y ~., family="binomial",data=wine)
```
\begin{itemize}

\item $\hat{\beta_1}$ and $\hat{\beta_2}$ can be interpreted as how the odds vary. The odds is given as $\frac{p_i}{1-p_i}$. As shown in the class notes, if the covariate $x_{1i}$ is increased by one unit, the odds is multiplied by $exp(\beta_1)$. The same is true for $x_{1i}$ and $exp(\beta_1)$. $\hat{\beta_1}$ and $\hat{\beta_2}$ are estimators of $\beta_1$ and $\beta_2$ and are estimated using the training data.

\item We find the formula for the class boundary by solving $\hat{Pr}(Y=1|\mathbf{x})=0.5$. This gives 
$$
  \frac{e^{\hat{\beta_0}+\hat{\beta_1}x_{i1}+\hat{\beta_2} x_{i2}}}{1+e^{\hat{\beta_0}+\hat{\beta_1}x_{i1}+\hat{\beta_2} x_{i2}}} = 0.5,$$so$$
  0.5e^{\hat{\beta_0}+\hat{\beta_1}x_{i1}+\hat{\beta_2} x_{i2}}=0.5.
$$
This means that we need $\hat{\beta_0}+\hat{\beta_1}x_{i1}+\hat{\beta_2}x_{i2} = 0$. Thus
$$\mathbf{x}_2=-\frac{\hat{\beta_0}}{\hat{\beta_2}} - \frac{\hat{\beta_1}}{\hat{\beta_2}}\mathbf{x}_1,$$
and the formula is linear.

\item The training data is plotted with the class boundary.

\end{itemize}

```{r, echo = FALSE}
slope = -fit$coefficients[2] / fit$coefficients[3]
intercept = -fit$coefficients[1] / fit$coefficients[3]
g1 = ggplot(data = train, aes(x = x1, y = x2, color = y)) + geom_point(data = train, pch = 3) + geom_abline(slope=slope,intercept=intercept)+ggtitle("Training data and logistic boundary")
print(g1)
print(slope)
print(intercept)
```

\begin{itemize}
\item From the summary we find that $\hat{\beta_0}= 3.184$, $\hat{\beta_1}= 0.265$, $\hat{\beta_2}=-1.890 $. The probability of class 1 given $x_1=17$ and $x_2=3$ is then
$$
p=\frac{e^{\hat{\beta_0}+\hat{\beta_1}x_{1}+\hat{\beta_2} x_{2}}}{1+e^{\hat{\beta_0}+\hat{\beta_1}x_{1}+\hat{\beta_2} x_{2}}} = 0.882.
$$
The interpretation of this is that based on the stimated $\beta$-values, the probability of class one given $\mathbf{x}$ is $88.2%$.
\end{itemize}

```{r, echo = FALSE}
X = data.matrix(test)
Y <- X[,1]
X[,1] = 1
betahat = as.matrix( fit$coefficients )
Yhat = X %*% betahat
pred = exp(Yhat) / (1 + exp(Yhat))
testclass = ifelse(pred > 0.5, 1, 0)
t = table(test$y, testclass)
```

* The predicted probabilities for the test set is visualized in a confusion matrix with a cutoff of $0.5$.



