---
subtitle: "TMA4268 Statistical Learning V2018"
title: "Compulsory exercise 1: Group X"
author: "NN1, NN2 and NN3"
date: "Date when you hand in"
output: #3rd letter intentation hierarchy
  # prettydoc::html_pretty:
  #   theme: tactile
  #   highlight: github
  pdf_document:
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(GGally)
library(class)
library(MASS)
library(pROC)
```

\section*{Problem 2 - Linear regression} 
```{r, echo = FALSE}
data = read.table("https://www.math.ntnu.no/emner/TMA4268/2018v/data/SYSBPreg3uid.txt")
modelA = lm(-1/sqrt(SYSBP) ~ ., data = data)
summary(modelA)
```

\section*{a)}

\begin{itemize}

\item The fittet model has the equation $\hat{Y} = X\hat{\beta}$, where $\hat{\beta} = (X^TX)^{-1}X^TY$, X is the $n \times (p + 1)$ design matrix, and Y is the corresponding response values.

\item "Estimate" is the estimated coeffictients obtaining the minimum residual square error with the data set. The "intercept" is the constant term in the regression model.

\item The "standard error" is the estimated standard deviation in the estimated coefficients. It is given as the square root of $\hat{Var}(\hat{{\beta}_{j}}) = c_{jj}{\hat{\sigma}}^2$, where $c_{ij} = ((X^TX)^{-1})_{ij}$ and ${\hat{\sigma}}^2 = (Y - \hat{Y})^T(Y - \hat{Y}) / (n - p - 1)$.

\item The "t value" is for every coefficent $j$, $\frac{ \hat{{\beta}_j} }{ \sqrt{c_{jj}} \hat{ \sigma } }$ which is t distributed with $n - p - 1$ degrees of freedom under the assumtion that $H_0$ is true, that is, ${\beta}_j$ truly is $0$. "Pr(t > |t|)" is then the probability of obeserving such an extreme t value given that $H_0$ is true. Hence Pr(>|t|) $:= P(|T_{n - p - 1}| \geq |\frac{ \hat{{\beta}_j} }{ \sqrt{c_{jj}} \hat{ \sigma } }|) = 2P(T_{n - p - 1} \geq |\frac{ \hat{{\beta}_j} }{ \sqrt{c_{jj}} \hat{ \sigma } }|)$.

\item The "Residual standard error" is our estimate for the variance of Y. The standard error squared is given as ${ \hat{\sigma} }^2 = (Y - \hat{Y})^T(Y - \hat{Y}) / (n - p - 1)$.

\item The "F - statistic" is used to check the hypothesis of all betas being 0. In the table it is given as $\frac{(TSS - RSS) / p} {RSS / (n - p - 1)}$, which is Fisher distributed with degrees of freedom $p$ and $n - p - 1$, where $TSS := \sum_{i = 1}^{n}(y_i - \bar{y})^2$, and $RSS := \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2$.

\end{itemize}

\subsection{b)}

\begin{itemize}

\item The proportion of variability explained by the model is given by the $R^2 -$ statistic $:= (TSS - RSS) / TSS$, here being equal to 0.2494. Hence our model explains approximately $25\%$ of the variance in the response value.

\end{itemize}
```{r, echo = FALSE}
# residuls vs fitted
p1 <- ggplot(modelA, aes(.fitted, .resid)) + geom_point(pch = 21) + 
  geom_hline(yintercept = 0, linetype = "dashed") + 
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") + 
  labs(x = "Fitted values", y = "Residuals", title = "Fitted values vs. residuals",
       subtitle = deparse(modelA$call))
# qq-plot of residuals
p2 <- ggplot(modelA, aes(sample = .stdresid)) +
  stat_qq(pch = 19) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals", title = "Normal Q-Q", subtitle = deparse(modelA$call))
p1
p2
```
\begin{itemize}

\item Looking at the plot of residuals vs. fitted values we note that it does not appear to be a correlation between the value of the response and the variance of the response, and the mean appears to be 0. This fits well with the assumtion of the noise being normally distributed with mean 0 and constant variance.

The QQ-plots strengthens our belief in this assumtion, as the points form a linear line.

\end{itemize}

```{r, echo = FALSE}
modelB <- lm(SYSBP ~ ., data = data)
summary(modelB)
# residuls vs fitted
p1 <- ggplot(modelB, aes(.fitted, .resid)) + geom_point(pch = 21) + 
  geom_hline(yintercept = 0, linetype = "dashed") + 
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") + 
  labs(x = "Fitted values", y = "Residuals", title = "Fitted values vs. residuals",
       subtitle = deparse(modelB$call))
# qq-plot of residuals
p2 <- ggplot(modelB, aes(sample = .stdresid)) +
  stat_qq(pch = 19) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals", title = "Normal Q-Q", subtitle = deparse(modelB$call))
p2
```

\begin{itemize}

\item The RSE is considerably larger for model B, that is, the estimated variance in both the response and our estimated coefficients are larger in this model. Looking at the diagnostic plots of model B we also note that the QQ-plot suggests that these residuals are not normally distributed. Clearly we prefer model A to make inference about systolic blood pressure, for this model brings more likely coefficent estimates and follows the noise assumptions better.

\end{itemize}

\subsection*{c)}

\begin{itemize}

\item The estimate for ${\hat{\beta}}_{BMI}$ is $3.087\cdot 10^{-4}$. 

\item We interpret the estimated coefficient ${\hat{\beta}}_{BMI}$ as the coefficient of the variable containing the value of BMI in the linear expression for $-1 / \sqrt{SYSBP}$, that is, the impact of change in BMI on the response

\[
{\hat{\beta}}_{BMI} = \frac{\partial (-1/\sqrt{SYSBP)}}{\partial BMI}
\]

\item Since ${\hat{\beta}}_{BMI} \sim N({\beta}_{BMI}, {\sigma}^2c_{BMI})$, where $c_{BMI} := \textrm{diagonal entry corresponding to BMI of }(X^TX)^{-1}$ we have

\[
\frac{ {(\hat{\beta}}_{BMI} - {\beta}_{BMI}) / (\sigma \sqrt{c_{BMI}}) }{ \sqrt{ \frac{1}{{\sigma}^2} RSS / (n - p - 1) } } = \frac{ {\hat{\beta}}_{BMI} - {\beta}_{BMI} }{\sqrt{\frac{RSS}{n - p - 1}c_{BMI}}} \sim T_{n - p - 1}
\]

It follows that

\[
Pr({\beta}_{BMI} \in ({\hat{\beta}}_{BMI} - {\hat{\sigma}} \sqrt{c_{BMI}} t_{0.995, 2593}, {\hat{\beta}}_{BMI} - {\hat{\sigma}} \sqrt{c_{BMI}} t_{0.005, 2593} )) = 0.99
\]

Setting $t_{0.005, 2593} = -2.577727$ and $t_{0.995, 2593} = 2.577727$, we compute the interval to be $(2.325282\cdot 10^{-4}, 3.848718\cdot 10^{-4})$. This interval tells us that with probability $0.99$, the true value of the coefficient is contained in this interval.

\item We note that if $H_0$ is true, the center of the t distribution for prediction of $\hat{\beta}_{BMI}$ would be $0$, but the degrees of freedom the same as for this prediction. Hence, a $99\%$ prediction interval for the estimated coefficient would in this case be $(-|2.325282\cdot 10^{-4} -  3.087\cdot 10^{-4}|, |3.848718\cdot 10^{-4} - 3.087\cdot 10^{-4}|) = (-7.61718\cdot 10^{-5}, 7.61718\cdot 10^{-5})$. Clearly our observed value is outside the interval, meaning that the p value must be less than or equal to $0.01$.

\end{itemize}

\section*{d)}

\begin{itemize}

\item Model A predicts the response of these values to be $-0.08667246$, which corresponds to a SYSBP of $133.1183$. 

\item Let $\tilde{Y_0}$ be a new observation of $-1 / \sqrt{SYSBP}$ corresponding to the point ${x_0}$. Since we have $\tilde{Y_0} - {x_0}^T\beta \sim N(0, {\sigma}^2(1 + {x_0}^T(X^TX)^{-1}{x_0}))$ we get

\[
\frac{(\tilde{Y_0} - {x_0}^T\hat{\beta}) / (\sigma \sqrt{1 + {x_0}^T(X^TX)^{-1}x_0})}{\sqrt{\frac{1}{{\sigma}^2} RSS / (n - p - 1)}} = \frac{ \tilde{Y_0} - {x_0}^T\hat{\beta} }{ \hat{\sigma}\sqrt{1 + {x_0}^T(X^TX)^{-1}x_0} } \sim T_{n - p - 1}
\]

letting $\tilde{Y_0} = -\frac{1}{\sqrt{Y_0}}$ we obtain the following prediction interval for SYSBP at $x_0$

\[
Pr(Y_0 \in \bigg( \frac{1} {({x_0}^T\hat{\beta} + \hat{\sigma}kt_{0.05, 2593})^2 }, \frac{1} {({x_0}^T\hat{\beta} + \hat{\sigma} kt_{0.95, 2593})^2}\bigg) ) = 0.90, \\ k = \sqrt{1 + {x_0}^T(X^TX)^{-1}x_0}
\]

Setting $t_{0.05, 2593} = -1.645441$ and $t_{0.95, 2593} = 1.645441$ we compute the following prediction interval $(107.9250, 168.2845)$. 

\item ??????????????????????????

\end{itemize}

\section*{Problem 3 - Classification}

\section*{a)}

\begin{itemize}

\item We want to show that $logit(p_i)=log(\frac{p_i}{1-p_i})$ is a linear function, where $p_i=\frac{e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}{1+e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}$. We see that 

\[
1-p_i=1-\frac{e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}{1+e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}=\frac{1}{1+e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}.
\]

and thus

\[
\textrm{logit}(p_i)=\log\bigg(\frac{p_i}{1-p_i}\bigg)=\log\Bigg(\frac{\frac{e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}{1+e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}}{\frac{1}{1+e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}}}}\Bigg)= \log(e^{\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}})=\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}.
\]

So $\textrm{logit}(p_i)$ is linear.

\end{itemize}

```{r, echo = FALSE, warning=FALSE, message=FALSE}
wine = read.csv("https://www.math.ntnu.no/emner/TMA4268/2018v/data/Comp1Wine.csv",sep=" ")
wine$class = as.factor(wine$class-1)
colnames(wine) = c("y","x1","x2")

n = dim(wine)[1]
set.seed(1729) 
ord = sample(1:n) #shuffle
test = wine[ord[1:(n/2)],]

train = wine[ord[((n/2)+1):n],]

fit <- glm(y ~., family="binomial",data=wine)
summary(fit)
```

\begin{itemize}

\item $\hat{\beta_1}$ and $\hat{\beta_2}$ can be interpreted as how the odds vary with $x_{i1}$, $x_{i2}$ respectively. The odds is given as $\frac{p_i}{1-p_i}$. If the covariate $x_{i1}$ is increased by one unit, the odds is multiplied by $exp(\beta_1)$. The same is true for $x_{i2}$ and $exp(\beta_2)$. $hat{\beta}_i, \ i = 0, 1, 2$ and are estimates for the parameters $\beta$ in the model, and are estimated by maximum likelihood on the intital data.

\item We find the formula for the class boundary by solving $\hat{Pr}(Y = 1| X) = 0.5$. This gives 

\[
  \frac{e^{\hat{\beta_0}+\hat{\beta_1}x_{i1}+\hat{\beta_2} x_{i2}}}{1+e^{\hat{\beta_0}+\hat{\beta_1}x_{i1}+\hat{\beta_2} x_{i2}}} = 0.5,
\]

so

\[
  0.5e^{\hat{\beta_0}+\hat{\beta_1}x_{i1}+\hat{\beta_2} x_{i2}}=0.5.
\]

This means that we need $\hat{\beta_0}+\hat{\beta_1}x_{i1}+\hat{\beta_2}x_{i2} = 0$. Thus

\[
x_2 = -\frac{\hat{\beta_0}}{\hat{\beta_2}} - \frac{\hat{\beta_1}}{\hat{\beta_2}}x_1,
\]

and we see that the boundary is linear.

\item The training data is plotted with the class boundary.

\end{itemize}

```{r, echo = FALSE}
slope = -fit$coefficients[2] / fit$coefficients[3]
intercept = -fit$coefficients[1] / fit$coefficients[3]
g1 = ggplot(data = train, aes(x = x1, y = x2, color = y)) + geom_point(data = train, pch = 3) +
  geom_abline(slope = slope, intercept = intercept) + ggtitle("Training data and logistic boundary")

g1
```

\begin{itemize}

\item From the summary we find that $\hat{\beta_0} = 3.184$, $\hat{\beta_1} = 0.265$, $\hat{\beta_2} = -1.890$. The probability of class 1 given $x_1 = 17$ and $x_2 = 3$ is then

\[
p = \frac{e^{\hat{\beta_0}+\hat{\beta_1}x_{1}+\hat{\beta_2} x_{2}}}{1+e^{\hat{\beta_0}+\hat{\beta_1}x_{1}+\hat{\beta_2} x_{2}}} = 0.882.
\]

The interpretation of this is that based on the model, the probability of this point belonging to class one is $88.2\%$.

\item The predicted probabilities for the test set is visualized in a confusion matrix with a cutoff of $0.5$.

\end{itemize}

```{r, echo = FALSE}
X = data.matrix(test)
Y <- X[,1]
X[,1] = 1
betahat = as.matrix(fit$coefficients)
dot = X %*% betahat
pred = exp(dot) / (1 + exp(dot))
testclass = ifelse(pred > 0.5, 1, 0)
t = table(test$y, testclass)

t
```


\section*{c)}

\begin{itemize}

\item ${\pi}_k$ is the probability of an observation being from class $k$, that is, the probability of getting a sample from a certain wine, wine $1$ or $2$ in this case. ${\mu}_k$ is the expected value of a point from class $k$, in our case the expected values of $(x1, x2)$ corresponding to wine $1$ and wine $2$. $\Sigma$ is the variance matrix of the distribution of a class, here assumed to be equal for every class. Hence we assume that the variance in observations of $(x1, x2)$ are the same for both wine $1$ and $2$. $f_k(x)$ is the distribution of points $(x1, x2)$, coming from class $k$, i. e. wine $1$ and $2$, which we assume takes the form of the normal distribution with mean ${\mu}_k$ and variance $\Sigma$.

\end{itemize}

```{r, include = FALSE}
X <- data.matrix(train)
n <- length(X[,1])
X1 = c() 
X2 = c()
for (i in 1:n) {
  if (X[i,1] == 1) {
    X1 = c(X1, X[i,2:3])
  } else {
    X2 = c(X2, X[i,2:3])
  }
}
X1 = matrix(X1, byrow = TRUE, ncol = 2)
X2 = matrix(X2, byrow = TRUE, ncol = 2)
n1 <- length(X1[,1]); n2 <- length(X2[,1])

mu1hat <- t(X1) %*% rep(1, n1) / n1
mu2hat <- t(X2) %*% rep(1, n2) / n2

X1 = X1 - matrix(rep(mu1hat, n1), byrow = TRUE, ncol = 2)
X2 = X2 - matrix(rep(mu2hat, n2), byrow = TRUE, ncol = 2)
S1 = t(X1) %*% X1
S2 = t(X2) %*% X2

S <- (S1 + S2) / (n - 2)
```

\begin{itemize}

\item To estimate ${\pi}_k$ we consider the proportion of observations coming from class $k$, that is, $\hat{\pi}_k = \frac{n_k}{n}$. We compute $\hat{\pi}_1 = 32 / 65 = 0.4923$ and $\hat{pi}_2 = 33 / 65 = 0.5077$. To estimate ${\mu_k}$ we consider the estimated mean of points coming from class $k$, that is $\hat{\mu}_k = \frac{1}{n_k} \sum_{i, y_i = k} x_i$, which we compute to be: $\hat{\mu}_1 = (16.7781, 5.4575)^T$, $\hat{\mu}_2 = (19.6879, 3.0536)^T$. To estimate $\Sigma$ we consider the estimated variance for each class, $\hat{\Sigma}_k := \frac{1}{n_k - 1}\sum_{i, y_i = k} (X_i - \hat{\mu}_k)(X_i - \hat{\mu}_k)^T$, and compute: 
\[
\hat{\Sigma} = \sum_{k = 1}^2 \frac{n_k - 1}{n - 2} \hat{\Sigma}_k = 
\begin{bmatrix}
  6.2014 & -0.4447 \\
  -0.4447 & 1.1678
\end{bmatrix}
\]

\item The desicion boundary is given by the equality $P(Y = 0 | X) = P(Y = 1 | X)$, that is

\[
\frac{\pi_0 f_0(x)}{\sum_{i = 0}^{1}\pi_kf_i(x)} = \frac{\pi_1 f_1(x)}{\sum_{i = 0}^{1}\pi_kf_i(x)}
\]

which simplifies to 

\[
\pi_0 \exp(-\frac{1}{2}(x - \mu_0)^T{\Sigma}^{-1}(x - \mu_0)) = \pi_1 \exp(-\frac{1}{2}(x - \mu_1)^T{\Sigma}^{-1}(x - \mu_1))
\]

taking the logarithm on both sides yields

\[
\log(\pi_0) - \frac{1}{2}({\mu_0}^T{\Sigma}^{-1}\mu_0 - 2{\mu_0}^T{\Sigma}^{-1}x + x^T{\Sigma}^{-1}x) = \log(\pi_1) - \frac{1}{2}({\mu_1}^T{\Sigma}^{-1}\mu_1 - 2{\mu_1}^T{\Sigma}^{-1}x + x^T{\Sigma}^{-1}x) 
\]

and finally

\[
\log(\pi_0) - \frac{1}{2}{\mu_0}^T{\Sigma}^{-1}\mu_0 + {\mu_0}^T{\Sigma}^{-1}x = {\delta}_0(x) = \log(\pi_1) - \frac{1}{2}{\mu_1}^T{\Sigma}^{-1}\mu_1 + {\mu_1}^T{\Sigma}^{-1}x = {\delta}_1(x)
\]

\item Let $\hat{\delta}_k(x) := \log(\hat{\pi}_k) - \frac{1}{2}(x - \hat{\mu}_k)^T{\hat{\Sigma}}^{-1}(x - \hat{\mu_k}))$, then this descision boundary is given by

\[
\hat{Pr}(Y = 1 | X) = \frac{\pi_1\hat{f}_1(x)}{\sum_{i = 0}^{1}\pi_i \hat{f}_i(x)} = \frac{\exp(\hat{\delta_1}(x))}{\sum_{i = 0}^1\exp(\hat{\delta_i}(x))} = 0.5
\]

which gives

\[
\exp(\hat{\delta}_1(x)) = \exp(\hat{\delta}_0(x)),
\]

and thus

\[
\hat{\delta}_0(x) = \hat{\delta}_1(x).
\]

So the boundary becomes

\[
\log(\hat{\pi_0}) + \frac{1}{2}{\hat{\mu_0}}^T{\Sigma}^{-1}\hat{\mu_0} - {\hat{\mu_0}}^T{\hat{\Sigma}}^{-1}x = \log(\hat{\pi_1}) + \frac{1}{2}{\hat{\mu_1}}^T{\Sigma}^{-1}\hat{\mu_1} - {\hat{\mu_1}}^T{\hat{\Sigma}}^{-1}x
\]

that is

\[
\frac{1}{2}({\hat{\mu}_0}^T {\hat{\Sigma}}^{-1}\hat{\mu_0} - {\hat{\mu}_1}^T {\hat{\Sigma}}^{-1}\hat{\mu_1}) + \log\bigg(\frac{\hat{\pi}_0}{\hat{\pi}_1}\bigg) + ({\hat{\mu}_1}^T{\hat{\Sigma}}^{-1} - {\hat{\mu}_0}^T{\hat{\Sigma}}^{-1})x = 0
\]

Inserting our obtained estimates from the training set we get the boudary

\[
x_2 = 0.1711x_1 + 1.1205
\]

\item The descision boundary with both the training and test observations is shown below (circles are from the training set)

\end{itemize}

```{r, echo = FALSE}
Sinv <- solve(S)
pi1hat <- n1 / n; pi2hat <- n2 / n
b <- 0.5 * (t(mu1hat) %*% Sinv %*% mu1hat - t(mu2hat) %*% Sinv %*% mu2hat) + log(pi1hat / pi2hat) 
a <- t(mu2hat) %*% Sinv - t(mu1hat) %*% Sinv
slope <- -a[1] / a[2]; intercept <- -b / a[2]
p <- ggplot() + geom_point(data = train, aes(x = x1, y = x2, color = y), pch = 1) +
  geom_point(data = test, aes(x = x1, y = x2, color = y), pch = 3) + geom_abline(slope = slope, intercept = intercept)

p
```

\begin{itemize}

\item Done below

\end{itemize}

```{r}
wine_lda <- lda(y ~ x1 + x2, data = train)
```

\begin{itemize}

\item The confusion table is show below. We get a sesitivity of $12 / 27 = 0.4444$, and a specificity of $18 / 38 = 0.4737$. The performance is bad compared to the logistic regression and KNN. Perhaps this method is not flexible enough for this dataset. 

\end{itemize}

```{r, echo = FALSE}
predicted = predict(wine_lda, data = test)$class
t = table(test$y, predicted)
t
```

\begin{itemize}

\item The most important diffence in regard to using LDA or QDA would be that with QDA we expect the variance of the classes to be different, and hence use different covariance matrices in their distributions. This allows for a more flexible fit to the data.

\end{itemize}

\section*{d)}



